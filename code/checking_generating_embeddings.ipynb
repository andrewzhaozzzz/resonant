{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoModel, TrainingArguments, Trainer, DataCollatorForLanguageModeling, AutoTokenizer, AdamW, get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_paras = None\n",
    "# filepath = str(os.path.dirname(os.path.realpath(__file__)))\n",
    "\n",
    "# if \"hert5583\" in filepath:\n",
    "#     data_dir = \"/data/inet-demtech/hert5583/embed_resonance_data/data\"\n",
    "#     finetuned_model_dir = \"/data/inet-demtech/hert5583/embed_resonance_models/who_leads_model_final/checkpoint-500000\"\n",
    "# else:\n",
    "data_dir = \"/Users/HannahBailey/Documents/GitHub/embedding_resonance/data\"\n",
    "finetuned_model_dir = \"/Users/HannahBailey/Documents/GitHub/embedding_resonance/model/who_leads_model_final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def compute_embeddings(pickle_path, col_name, model_directory, save_name, num_paras=None, batch_size=1000):\n",
    "    print('Started')\n",
    "\n",
    "    df = pd.read_pickle(pickle_path)\n",
    "    # convert all columns to string\n",
    "    # df = df.astype(str)\n",
    "    print('Loaded dataframe')\n",
    "\n",
    "    if num_paras is not None:\n",
    "        df = df.head(num_paras)\n",
    "\n",
    "    print(\"Computing embeddings for\", len(df), \"paragraphs\")\n",
    "    \n",
    "    paragraphs = df[col_name].tolist()\n",
    "    paragraphs = [' ' if para is None else para for para in paragraphs]\n",
    "\n",
    "    # load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "    model = AutoModel.from_pretrained(model_directory)\n",
    "\n",
    "    sentence_embedding_list = []\n",
    "    last_index = 0\n",
    "    batch_number = 0\n",
    "    for i in range(0, len(paragraphs), batch_size):\n",
    "        end_idx = min(i+batch_size, len(paragraphs))\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(paragraphs)-1)//batch_size + 1}\")\n",
    "        batch = paragraphs[i:end_idx]\n",
    "        encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        sentence_embedding_list.append(sentence_embeddings)\n",
    "\n",
    "        # Write embeddings to HDF5\n",
    "        if i == 0:\n",
    "            print(\"Creating dataset...\")\n",
    "            sentence_embeddings = torch.cat(sentence_embedding_list, dim=0)\n",
    "            # Create dataset for embeddings]\n",
    "            # Define the HDF5 file\n",
    "            with h5py.File(os.path.join(model_directory, f'{save_name}_embeddings.h5'), 'w') as h5f:\n",
    "                h5f.create_dataset('embeddings', data=sentence_embeddings.cpu().numpy(),\n",
    "                                maxshape=(len(df), sentence_embeddings.shape[1]), chunks=True)\n",
    "                for col in df:\n",
    "                    print(col)\n",
    "                    print(type(df[col][0]))\n",
    "                    h5f.create_dataset(col, data=df[col].iloc[last_index:end_idx].str.encode('utf-8').values.astype('S'),\n",
    "                        maxshape=(len(df),), chunks=True)\n",
    "            sentence_embedding_list = []\n",
    "            last_index = end_idx\n",
    "\n",
    "        elif batch_number % 50 == 0 or end_idx == len(paragraphs):\n",
    "            sentence_embeddings = torch.cat(sentence_embedding_list, dim=0)\n",
    "            print(\"Saving dataset...\")\n",
    "            # Append to dataset for embeddings\n",
    "            with h5py.File(os.path.join(model_directory, f'{save_name}_embeddings.h5'), 'a') as h5f:\n",
    "                h5f['embeddings'].resize((h5f['embeddings'].shape[0] + sentence_embeddings.shape[0]), axis=0)\n",
    "                h5f['embeddings'][-sentence_embeddings.shape[0]:] = sentence_embeddings.cpu().numpy()\n",
    "\n",
    "                for col in df:\n",
    "                    h5f[col].resize((h5f[col].shape[0] + sentence_embeddings.shape[0]), axis=0)\n",
    "                    h5f[col][-sentence_embeddings.shape[0]:] = df[col].iloc[last_index:end_idx].str.encode('utf-8').values.astype('S')\n",
    "            sentence_embedding_list = []\n",
    "            last_index = end_idx\n",
    "\n",
    "        batch_number += 1\n",
    "\n",
    "        print(f\"Completed step for batch {i//batch_size + 1}\")\n",
    "\n",
    "        # Clear memory\n",
    "        del encoded_input, model_output, sentence_embeddings\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"All embeddings computed and saved to HDF5\")\n",
    "\n",
    "    # Save the final DataFrame\n",
    "    final_save_path = os.path.join(model_directory, f'{save_name}_tuned_embeddings_final.pkl')\n",
    "    df.to_pickle(final_save_path)\n",
    "    print(df.head())\n",
    "    print(\"All embeddings computed and saved to\", final_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n",
      "Loaded dataframe\n",
      "Computing embeddings for 35727478 paragraphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/HannahBailey/Documents/GitHub/embedding_resonance/model/who_leads_model_final were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at /Users/HannahBailey/Documents/GitHub/embedding_resonance/model/who_leads_model_final and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/3572748\n",
      "Creating dataset...\n",
      "user_type\n",
      "<class 'str'>\n",
      "date\n",
      "<class 'str'>\n",
      "post_text\n",
      "<class 'str'>\n",
      "Completed step for batch 1\n",
      "Processing batch 2/3572748\n",
      "Completed step for batch 2\n",
      "Processing batch 3/3572748\n",
      "Completed step for batch 3\n",
      "Processing batch 4/3572748\n",
      "Completed step for batch 4\n",
      "Processing batch 5/3572748\n",
      "Completed step for batch 5\n",
      "Processing batch 6/3572748\n",
      "Saving dataset...\n",
      "Completed step for batch 6\n",
      "Processing batch 7/3572748\n",
      "Completed step for batch 7\n",
      "Processing batch 8/3572748\n",
      "Completed step for batch 8\n",
      "Processing batch 9/3572748\n",
      "Completed step for batch 9\n",
      "Processing batch 10/3572748\n",
      "Completed step for batch 10\n",
      "Processing batch 11/3572748\n",
      "Saving dataset...\n",
      "Completed step for batch 11\n",
      "Processing batch 12/3572748\n",
      "Completed step for batch 12\n",
      "Processing batch 13/3572748\n",
      "Completed step for batch 13\n",
      "Processing batch 14/3572748\n",
      "Completed step for batch 14\n",
      "Processing batch 15/3572748\n",
      "Completed step for batch 15\n",
      "Processing batch 16/3572748\n",
      "Saving dataset...\n",
      "Completed step for batch 16\n",
      "Processing batch 17/3572748\n",
      "Completed step for batch 17\n",
      "Processing batch 18/3572748\n",
      "Completed step for batch 18\n",
      "Processing batch 19/3572748\n",
      "Completed step for batch 19\n",
      "Processing batch 20/3572748\n",
      "Completed step for batch 20\n",
      "Processing batch 21/3572748\n",
      "Saving dataset...\n",
      "Completed step for batch 21\n",
      "Processing batch 22/3572748\n",
      "Completed step for batch 22\n",
      "Processing batch 23/3572748\n",
      "Completed step for batch 23\n",
      "Processing batch 24/3572748\n",
      "Completed step for batch 24\n",
      "Processing batch 25/3572748\n",
      "Completed step for batch 25\n",
      "Processing batch 26/3572748\n",
      "Saving dataset...\n",
      "Completed step for batch 26\n",
      "Processing batch 27/3572748\n",
      "Completed step for batch 27\n",
      "Processing batch 28/3572748\n",
      "Completed step for batch 28\n",
      "Processing batch 29/3572748\n",
      "Completed step for batch 29\n",
      "Processing batch 30/3572748\n",
      "Completed step for batch 30\n",
      "Processing batch 31/3572748\n",
      "Saving dataset...\n",
      "Completed step for batch 31\n",
      "Processing batch 32/3572748\n",
      "Completed step for batch 32\n",
      "Processing batch 33/3572748\n",
      "Completed step for batch 33\n",
      "Processing batch 34/3572748\n",
      "Completed step for batch 34\n",
      "Processing batch 35/3572748\n",
      "Completed step for batch 35\n",
      "Processing batch 36/3572748\n",
      "Saving dataset...\n",
      "Completed step for batch 36\n",
      "Processing batch 37/3572748\n",
      "Completed step for batch 37\n",
      "Processing batch 38/3572748\n",
      "Completed step for batch 38\n",
      "Processing batch 39/3572748\n",
      "Completed step for batch 39\n",
      "Processing batch 40/3572748\n",
      "Completed step for batch 40\n",
      "Processing batch 41/3572748\n",
      "Saving dataset...\n",
      "Completed step for batch 41\n",
      "Processing batch 42/3572748\n",
      "Completed step for batch 42\n",
      "Processing batch 43/3572748\n",
      "Completed step for batch 43\n",
      "Processing batch 44/3572748\n",
      "Completed step for batch 44\n",
      "Processing batch 45/3572748\n",
      "Completed step for batch 45\n",
      "Processing batch 46/3572748\n",
      "Saving dataset...\n",
      "Completed step for batch 46\n",
      "Processing batch 47/3572748\n",
      "Completed step for batch 47\n",
      "Processing batch 48/3572748\n",
      "Completed step for batch 48\n",
      "Processing batch 49/3572748\n",
      "Completed step for batch 49\n",
      "Processing batch 50/3572748\n",
      "Completed step for batch 50\n",
      "Processing batch 51/3572748\n",
      "Saving dataset...\n",
      "Completed step for batch 51\n",
      "Processing batch 52/3572748\n",
      "Completed step for batch 52\n",
      "Processing batch 53/3572748\n",
      "Completed step for batch 53\n",
      "Processing batch 54/3572748\n",
      "Completed step for batch 54\n",
      "Processing batch 55/3572748\n",
      "Completed step for batch 55\n",
      "Processing batch 56/3572748\n",
      "Saving dataset...\n",
      "Completed step for batch 56\n",
      "Processing batch 57/3572748\n",
      "Completed step for batch 57\n",
      "Processing batch 58/3572748\n",
      "Completed step for batch 58\n",
      "Processing batch 59/3572748\n",
      "Completed step for batch 59\n",
      "Processing batch 60/3572748\n",
      "Completed step for batch 60\n",
      "Processing batch 61/3572748\n",
      "Saving dataset...\n",
      "Completed step for batch 61\n",
      "Processing batch 62/3572748\n",
      "Completed step for batch 62\n",
      "Processing batch 63/3572748\n",
      "Completed step for batch 63\n",
      "Processing batch 64/3572748\n",
      "Completed step for batch 64\n",
      "Processing batch 65/3572748\n",
      "Completed step for batch 65\n",
      "Processing batch 66/3572748\n",
      "Saving dataset...\n",
      "Completed step for batch 66\n",
      "Processing batch 67/3572748\n",
      "Completed step for batch 67\n",
      "Processing batch 68/3572748\n",
      "Completed step for batch 68\n",
      "Processing batch 69/3572748\n",
      "Completed step for batch 69\n",
      "Processing batch 70/3572748\n",
      "Completed step for batch 70\n",
      "Processing batch 71/3572748\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-58f3b6ee7a93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwho_leads_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'who_leads_who_follows'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpickle_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwho_leads_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cleaned_who_leads_df.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcompute_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post_text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinetuned_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'who_leads_model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_paras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_paras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-75079870f576>\u001b[0m in \u001b[0;36mcompute_embeddings\u001b[0;34m(pickle_path, col_name, model_directory, save_name, num_paras, batch_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0msentence_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    848\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         )\n\u001b[0;32m--> 850\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    851\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 )\n\u001b[1;32m    523\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    525\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2360\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "who_leads_folder = os.path.join(data_dir, 'who_leads_who_follows')  \n",
    "pickle_path = os.path.join(who_leads_folder, 'cleaned_who_leads_df.pkl')\n",
    "compute_embeddings(pickle_path=pickle_path, col_name='post_text', model_directory=finetuned_model_dir, save_name='who_leads_model', num_paras=num_paras, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['date', 'embeddings', 'post_text', 'user_type']>\n",
      "Shape of embeddings: (660, 768)\n",
      "First five embeddings:\n",
      " [[-0.0156679   0.00471961 -0.00389284 ...  0.05953515 -0.02004566\n",
      "  -0.03649166]\n",
      " [-0.00964241 -0.02049181 -0.00454568 ...  0.03874613 -0.06312865\n",
      "  -0.02873504]\n",
      " [-0.02146283  0.03928605 -0.00450213 ...  0.028684   -0.02098445\n",
      "  -0.05096287]\n",
      " [-0.03896829  0.00304933 -0.00493355 ...  0.02121274 -0.03294142\n",
      "  -0.02406158]\n",
      " [-0.04928146  0.07072747 -0.00353959 ...  0.04813822 -0.00219928\n",
      "  -0.02595499]]\n"
     ]
    }
   ],
   "source": [
    "# optional - check that the embeddings were saved correctly\n",
    "# Path to your HDF5 file\n",
    "hdf5_file_path = '/Users/HannahBailey/Documents/GitHub/embedding_resonance/model/who_leads_model_final/who_leads_model_embeddings.h5'\n",
    "\n",
    "# Open the HDF5 file in read mode\n",
    "with h5py.File(hdf5_file_path, 'r') as file:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % file.keys())\n",
    "    # Get the embeddings dataset\n",
    "    embeddings = file['embeddings']\n",
    "    print(\"Shape of embeddings:\", embeddings.shape)\n",
    "    \n",
    "    # Optionally, read a subset of embeddings into memory\n",
    "    # Here, we read the first 5 embeddings\n",
    "    first_five_embeddings = embeddings[:5]\n",
    "    print(\"First five embeddings:\\n\", first_five_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
