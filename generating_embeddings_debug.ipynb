{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hannah/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state  # token-level embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def compute_embeddings(pickle_path, col_name, model_directory, tokenizer_path, save_name, batch_size=256, checkpoint_interval=100):\n",
    "    df = pd.read_pickle(pickle_path)\n",
    "    documents = df[col_name].fillna(\" \").tolist()\n",
    "    print(f\"Computing embeddings for {len(documents)} documents\")\n",
    "\n",
    "    # Load tokenizer from the provided tokenizer path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    model = AutoModel.from_pretrained(model_directory)\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Create the embedding_output folder if it does not exist\n",
    "    output_dir = os.path.join(model_directory, \"embedding_output\")\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    all_embeddings = []\n",
    "    num_batches = (len(documents) - 1) // batch_size + 1\n",
    "\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        encoded = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded)\n",
    "        embeddings = mean_pooling(model_output, encoded['attention_mask'])\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "        batch_num = i // batch_size + 1\n",
    "        print(f\"Processed batch {batch_num}/{num_batches}\")\n",
    "        del encoded, model_output, embeddings\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save checkpoint periodically or on the last batch\n",
    "        if batch_num % checkpoint_interval == 0 or batch_num == num_batches:\n",
    "            checkpoint_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "            checkpoint_path = os.path.join(output_dir, f\"{save_name}_embeddings_checkpoint_{batch_num}.pt\")\n",
    "            torch.save(checkpoint_embeddings, checkpoint_path)\n",
    "            print(f\"Saved checkpoint at batch {batch_num} to {checkpoint_path}\")\n",
    "\n",
    "    # Final save if last batch is not a checkpoint\n",
    "    if batch_num % checkpoint_interval != 0:\n",
    "        final_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "        final_path = os.path.join(output_dir, f\"{save_name}_embeddings.pt\")\n",
    "        torch.save(final_embeddings, final_path)\n",
    "        print(f\"Saved final embeddings to {final_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for 35727478 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at /home/hannah/github/embedding_resonance/model/who_leads_model_final/checkpoint-100000 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/139561\n",
      "Processed batch 2/139561\n",
      "Processed batch 3/139561\n",
      "Processed batch 4/139561\n",
      "Processed batch 5/139561\n",
      "Processed batch 6/139561\n",
      "Processed batch 7/139561\n",
      "Processed batch 8/139561\n",
      "Processed batch 9/139561\n",
      "Processed batch 10/139561\n",
      "Processed batch 11/139561\n",
      "Processed batch 12/139561\n",
      "Processed batch 13/139561\n",
      "Processed batch 14/139561\n",
      "Processed batch 15/139561\n",
      "Processed batch 16/139561\n",
      "Processed batch 17/139561\n",
      "Processed batch 18/139561\n",
      "Processed batch 19/139561\n",
      "Processed batch 20/139561\n",
      "Processed batch 21/139561\n",
      "Processed batch 22/139561\n",
      "Processed batch 23/139561\n",
      "Processed batch 24/139561\n",
      "Processed batch 25/139561\n",
      "Processed batch 26/139561\n",
      "Processed batch 27/139561\n",
      "Processed batch 28/139561\n",
      "Processed batch 29/139561\n",
      "Processed batch 30/139561\n",
      "Processed batch 31/139561\n",
      "Processed batch 32/139561\n",
      "Processed batch 33/139561\n",
      "Processed batch 34/139561\n",
      "Processed batch 35/139561\n",
      "Processed batch 36/139561\n",
      "Processed batch 37/139561\n",
      "Processed batch 38/139561\n",
      "Processed batch 39/139561\n",
      "Processed batch 40/139561\n",
      "Processed batch 41/139561\n",
      "Processed batch 42/139561\n",
      "Processed batch 43/139561\n",
      "Processed batch 44/139561\n",
      "Processed batch 45/139561\n",
      "Processed batch 46/139561\n",
      "Processed batch 47/139561\n",
      "Processed batch 48/139561\n",
      "Processed batch 49/139561\n",
      "Processed batch 50/139561\n",
      "Processed batch 51/139561\n",
      "Processed batch 52/139561\n",
      "Processed batch 53/139561\n",
      "Processed batch 54/139561\n",
      "Processed batch 55/139561\n",
      "Processed batch 56/139561\n",
      "Processed batch 57/139561\n",
      "Processed batch 58/139561\n",
      "Processed batch 59/139561\n",
      "Processed batch 60/139561\n",
      "Processed batch 61/139561\n",
      "Processed batch 62/139561\n",
      "Processed batch 63/139561\n",
      "Processed batch 64/139561\n",
      "Processed batch 65/139561\n",
      "Processed batch 66/139561\n",
      "Processed batch 67/139561\n",
      "Processed batch 68/139561\n",
      "Processed batch 69/139561\n",
      "Processed batch 70/139561\n",
      "Processed batch 71/139561\n",
      "Processed batch 72/139561\n",
      "Processed batch 73/139561\n",
      "Processed batch 74/139561\n",
      "Processed batch 75/139561\n",
      "Processed batch 76/139561\n",
      "Processed batch 77/139561\n",
      "Processed batch 78/139561\n",
      "Processed batch 79/139561\n",
      "Processed batch 80/139561\n",
      "Processed batch 81/139561\n",
      "Processed batch 82/139561\n",
      "Processed batch 83/139561\n",
      "Processed batch 84/139561\n",
      "Processed batch 85/139561\n",
      "Processed batch 86/139561\n",
      "Processed batch 87/139561\n",
      "Processed batch 88/139561\n",
      "Processed batch 89/139561\n",
      "Processed batch 90/139561\n",
      "Processed batch 91/139561\n",
      "Processed batch 92/139561\n",
      "Processed batch 93/139561\n",
      "Processed batch 94/139561\n",
      "Processed batch 95/139561\n",
      "Processed batch 96/139561\n",
      "Processed batch 97/139561\n",
      "Processed batch 98/139561\n",
      "Processed batch 99/139561\n",
      "Processed batch 100/139561\n",
      "Saved checkpoint at batch 100 to /home/hannah/github/embedding_resonance/model/who_leads_model_final/checkpoint-100000/embedding_output/who_leads_model_embeddings_checkpoint_100.pt\n",
      "Processed batch 101/139561\n",
      "Processed batch 102/139561\n",
      "Processed batch 103/139561\n",
      "Processed batch 104/139561\n",
      "Processed batch 105/139561\n",
      "Processed batch 106/139561\n",
      "Processed batch 107/139561\n",
      "Processed batch 108/139561\n",
      "Processed batch 109/139561\n",
      "Processed batch 110/139561\n",
      "Processed batch 111/139561\n",
      "Processed batch 112/139561\n",
      "Processed batch 113/139561\n",
      "Processed batch 114/139561\n",
      "Processed batch 115/139561\n",
      "Processed batch 116/139561\n",
      "Processed batch 117/139561\n",
      "Processed batch 118/139561\n",
      "Processed batch 119/139561\n",
      "Processed batch 120/139561\n",
      "Processed batch 121/139561\n",
      "Processed batch 122/139561\n",
      "Processed batch 123/139561\n",
      "Processed batch 124/139561\n",
      "Processed batch 125/139561\n",
      "Processed batch 126/139561\n",
      "Processed batch 127/139561\n",
      "Processed batch 128/139561\n",
      "Processed batch 129/139561\n",
      "Processed batch 130/139561\n",
      "Processed batch 131/139561\n",
      "Processed batch 132/139561\n",
      "Processed batch 133/139561\n",
      "Processed batch 134/139561\n",
      "Processed batch 135/139561\n",
      "Processed batch 136/139561\n",
      "Processed batch 137/139561\n",
      "Processed batch 138/139561\n",
      "Processed batch 139/139561\n",
      "Processed batch 140/139561\n",
      "Processed batch 141/139561\n",
      "Processed batch 142/139561\n",
      "Processed batch 143/139561\n",
      "Processed batch 144/139561\n",
      "Processed batch 145/139561\n",
      "Processed batch 146/139561\n",
      "Processed batch 147/139561\n",
      "Processed batch 148/139561\n",
      "Processed batch 149/139561\n",
      "Processed batch 150/139561\n",
      "Processed batch 151/139561\n",
      "Processed batch 152/139561\n",
      "Processed batch 153/139561\n",
      "Processed batch 154/139561\n",
      "Processed batch 155/139561\n",
      "Processed batch 156/139561\n",
      "Processed batch 157/139561\n",
      "Processed batch 158/139561\n",
      "Processed batch 159/139561\n",
      "Processed batch 160/139561\n",
      "Processed batch 161/139561\n",
      "Processed batch 162/139561\n",
      "Processed batch 163/139561\n",
      "Processed batch 164/139561\n",
      "Processed batch 165/139561\n",
      "Processed batch 166/139561\n",
      "Processed batch 167/139561\n",
      "Processed batch 168/139561\n",
      "Processed batch 169/139561\n",
      "Processed batch 170/139561\n",
      "Processed batch 171/139561\n",
      "Processed batch 172/139561\n",
      "Processed batch 173/139561\n",
      "Processed batch 174/139561\n",
      "Processed batch 175/139561\n",
      "Processed batch 176/139561\n",
      "Processed batch 177/139561\n",
      "Processed batch 178/139561\n",
      "Processed batch 179/139561\n",
      "Processed batch 180/139561\n",
      "Processed batch 181/139561\n",
      "Processed batch 182/139561\n",
      "Processed batch 183/139561\n",
      "Processed batch 184/139561\n",
      "Processed batch 185/139561\n",
      "Processed batch 186/139561\n",
      "Processed batch 187/139561\n",
      "Processed batch 188/139561\n",
      "Processed batch 189/139561\n",
      "Processed batch 190/139561\n",
      "Processed batch 191/139561\n",
      "Processed batch 192/139561\n",
      "Processed batch 193/139561\n",
      "Processed batch 194/139561\n",
      "Processed batch 195/139561\n",
      "Processed batch 196/139561\n",
      "Processed batch 197/139561\n",
      "Processed batch 198/139561\n",
      "Processed batch 199/139561\n",
      "Processed batch 200/139561\n",
      "Saved checkpoint at batch 200 to /home/hannah/github/embedding_resonance/model/who_leads_model_final/checkpoint-100000/embedding_output/who_leads_model_embeddings_checkpoint_200.pt\n",
      "Processed batch 201/139561\n",
      "Processed batch 202/139561\n",
      "Processed batch 203/139561\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m who_leads_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwho_leads_who_follows\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m pickle_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(who_leads_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_who_leads_df.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mcompute_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpickle_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwho_leads_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m, in \u001b[0;36mcompute_embeddings\u001b[0;34m(pickle_path, col_name, model_directory, tokenizer_path, save_name, batch_size, checkpoint_interval)\u001b[0m\n\u001b[1;32m     32\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m mean_pooling(model_output, encoded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     33\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(embeddings, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m all_embeddings\u001b[38;5;241m.\u001b[39mappend(\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     35\u001b[0m batch_num \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        filepath = os.path.dirname(os.path.realpath(__file__))\n",
    "    except NameError:\n",
    "        filepath = os.getcwd()\n",
    "    if \"hbailey\" in filepath:\n",
    "        data_dir = \"/home/export/hbailey/data/embedding_resonance/data\"\n",
    "        model_directory = \"/home/export/hbailey/models/embedding_resonance/who_leads_model_final/checkpoint-100000\"\n",
    "        tokenizer_path = \"/home/export/hbailey/models/embedding_resonance/tokenizer\"\n",
    "    else:\n",
    "        data_dir = \"/home/hannah/github/embedding_resonance/data\"\n",
    "        model_directory = \"/home/hannah/github/embedding_resonance/model/who_leads_model_final/checkpoint-100000\"\n",
    "        tokenizer_path = \"/home/hannah/github/embedding_resonance/model/tokenizer\"\n",
    "\n",
    "    who_leads_folder = os.path.join(data_dir, 'who_leads_who_follows')\n",
    "    pickle_path = os.path.join(who_leads_folder, 'cleaned_who_leads_df.pkl')\n",
    "    compute_embeddings(pickle_path, 'post_text', model_directory, tokenizer_path, \"who_leads_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
